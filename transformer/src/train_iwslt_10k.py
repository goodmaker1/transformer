# train_iwslt_10k.py
import os, math, argparse, csv, random
from typing import List, Dict
import numpy as np
from tqdm import tqdm

import torch
import torch.nn as nn
import torch.nn.functional as F

from datasets import load_dataset
from transformers import AutoTokenizer

# ä½ è‡ªå·±çš„å®žçŽ°ï¼ˆç¡®ä¿ transformer.py ä¸­æš´éœ²è¿™ä¸¤ä¸ªç¬¦å·ï¼‰
from transformer import BaseTransformer, pad_mask


# -----------------------------
# å·¥å…·
# -----------------------------
def set_seed(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


def batch_iter(dataset, tokenizer, batch_size: int, max_len: int, device, shuffle=True):
    """
    dataset: HF Dataset åˆ‡ç‰‡åŽå¯¹è±¡ï¼ˆåŒ…å« 'translation': {'en': ..., 'de': ...}ï¼‰
    """
    idx = np.arange(len(dataset))
    if shuffle:
        np.random.shuffle(idx)

    for s in range(0, len(idx), batch_size):
        sub = [dataset[int(i)] for i in idx[s : s + batch_size]]
        src_texts = [ex["translation"]["en"] for ex in sub]
        trg_texts = [ex["translation"]["de"] for ex in sub]

        src_enc = tokenizer(
            src_texts, padding="max_length", truncation=True, max_length=max_len, return_tensors="pt"
        )
        trg_enc = tokenizer(
            trg_texts, padding="max_length", truncation=True, max_length=max_len, return_tensors="pt"
        )

        src_ids = src_enc.input_ids.to(device)
        trg_ids = trg_enc.input_ids.to(device)
        yield src_ids, trg_ids


@torch.no_grad()
def evaluate(model, dataset, tokenizer, pad_idx, batch_size, max_len, device):
    model.eval()
    total_loss, total_tokens = 0.0, 0

    for src, trg in batch_iter(dataset, tokenizer, batch_size, max_len, device, shuffle=False):
        # maskï¼ˆè‹¥ä½ çš„ forward å†…éƒ¨è‡ªå·±å¤„ç†ï¼Œä¹Ÿå¯ä»¥ä¸æ˜¾å¼ä¼ ï¼‰
        _src_mask, _trg_mask = pad_mask(src, trg, pad_idx)

        logits = model(src, trg)
        log_probs = F.log_softmax(logits, dim=-1)
        loss = F.nll_loss(
            log_probs.view(-1, log_probs.size(-1)),
            trg.view(-1),
            ignore_index=pad_idx,
            reduction="sum",
        )
        tokens = (trg != pad_idx).sum().item()
        total_loss += loss.item()
        total_tokens += tokens

    model.train()
    return total_loss / max(1, total_tokens)


# -----------------------------
# ä¸»ç¨‹åº
# -----------------------------
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--train_size", type=int, default=10_000, help="è®­ç»ƒæ ·æœ¬æ•°ï¼ˆå›ºå®šå–å‰ N æ¡ï¼‰")
    parser.add_argument("--val_size", type=int, default=2_000, help="éªŒè¯æ ·æœ¬æ•°ï¼ˆå›ºå®šå–å‰ N æ¡ï¼‰")
    parser.add_argument("--seq_len", type=int, default=128)
    parser.add_argument("--batch_size", type=int, default=32)
    parser.add_argument("--epochs", type=int, default=8)
    parser.add_argument("--d_model", type=int, default=256)
    parser.add_argument("--d_ff", type=int, default=512)
    parser.add_argument("--n_heads", type=int, default=8)  # â† 8å¤´
    parser.add_argument("--n_layers", type=int, default=2)
    parser.add_argument("--dropout", type=float, default=0.1)
    parser.add_argument("--lr", type=float, default=3e-4)
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--no_pos_enc", action="store_true", help="æ¶ˆèžï¼šå…³é—­ä½ç½®ç¼–ç ")
    parser.add_argument("--run_name", type=str, default="iwslt_10k_heads8")
    args = parser.parse_args()

    if torch.cuda.is_available():
        device = torch.device("cuda")
        gpu_name = torch.cuda.get_device_name(0)
        print(f"\nä½¿ç”¨è®¾å¤‡: CUDA ({gpu_name})")
        print(f"  å½“å‰ GPU æ•°é‡: {torch.cuda.device_count()}")
    else:
        device = torch.device("cpu")
        print("\nå½“å‰çŽ¯å¢ƒä¸æ”¯æŒ CUDAï¼Œä½¿ç”¨ CPU è¿›è¡Œè®­ç»ƒã€‚")

    set_seed(args.seed)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    os.makedirs("../results", exist_ok=True)

    # ------------------ æ•°æ®ï¼šä»…å– 10K/2K ------------------
    print("ðŸ“˜ Loading IWSLT2017 (enâ†’de)...")
    ds = load_dataset("iwslt2017", "iwslt2017-en-de")
    full_train, full_val = ds["train"], ds["validation"]

    # å›ºå®šå–å‰ N æ¡ï¼Œç¡®ä¿å°è§„æ¨¡å¯æŽ§
    train_ds = full_train.select(range(min(args.train_size, len(full_train))))
    val_ds = full_val.select(range(min(args.val_size, len(full_val))))

    # åˆ†è¯å™¨ï¼ˆå« pad/cls/sep/bos/eosï¼‰
    tokenizer = AutoTokenizer.from_pretrained("Helsinki-NLP/opus-mt-en-de")
    pad_idx = tokenizer.pad_token_id
    assert pad_idx is not None, "åˆ†è¯å™¨å¿…é¡»æä¾› pad_token_id"

    # ------------------ æ¨¡åž‹ ------------------
    class VocabProxy:
        n_vocabs = len(tokenizer)

    model = BaseTransformer(
        vocab=VocabProxy(),
        d_model=args.d_model,
        d_ff=args.d_ff,
        n_heads=args.n_heads,
        n_layers=args.n_layers,
        dropout=args.dropout,
        pad_idx=pad_idx,
    ).to(device)

    # æ¶ˆèžï¼šåŽ»ä½ç½®ç¼–ç 
    if args.no_pos_enc:
        class Identity(nn.Module):
            def forward(self, x, step=None): return x
        model.pos_embed = Identity()
        print("âš ï¸ Position Encoding removed (ablation).")

    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)
    criterion = nn.NLLLoss(ignore_index=pad_idx, reduction="sum")

    # è®°å½•æ–‡ä»¶
    metrics_csv = os.path.join("../results", f"{args.run_name}_metrics.csv")
    with open(metrics_csv, "w", newline="") as f:
        csv.writer(f).writerow(["epoch", "train_loss_token", "val_loss_token", "val_ppl"])

    # ------------------ è®­ç»ƒå¾ªçŽ¯ ------------------
    for epoch in range(1, args.epochs + 1):
        model.train()
        total_loss, total_tokens = 0.0, 0

        pbar = tqdm(
            batch_iter(train_ds, tokenizer, args.batch_size, args.seq_len, device, shuffle=True),
            total=(len(train_ds) + args.batch_size - 1) // args.batch_size,
            desc=f"Epoch {epoch}",
        )

        for src, trg in pbar:
            optimizer.zero_grad(set_to_none=True)

            # forward
            logits = model(src, trg)
            log_probs = F.log_softmax(logits, dim=-1)
            loss = F.nll_loss(
                log_probs.view(-1, log_probs.size(-1)),
                trg.view(-1),
                ignore_index=pad_idx,
                reduction="sum",
            )
            tokens = (trg != pad_idx).sum().item()
            (loss / tokens).backward()

            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            total_loss += loss.item()
            total_tokens += tokens
            pbar.set_postfix(train_ppl=math.exp(total_loss / max(1, total_tokens)))

        # éªŒè¯
        val_loss = evaluate(
            model, val_ds, tokenizer, pad_idx, args.batch_size, args.seq_len, device
        )
        val_ppl = math.exp(val_loss)
        train_loss_token = total_loss / max(1, total_tokens)
        print(f"Epoch {epoch}: train_loss/token={train_loss_token:.4f} | val_ppl={val_ppl:.3f}")

        with open(metrics_csv, "a", newline="") as f:
            csv.writer(f).writerow([epoch, train_loss_token, val_loss, val_ppl])

    # æœ€ç»ˆä¿å­˜
    torch.save(model.state_dict(), os.path.join("../results", f"{args.run_name}_final.pt"))
    print(f"âœ… Done. Metrics -> {metrics_csv}")


if __name__ == "__main__":
    main()
